\relax 
\providecommand\hyper@newdestlabel[2]{}
\catcode `"\active 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{abbrvnat}
\babel@aux{english}{}
\babel@aux{english}{}
\babel@aux{russian}{}
\babel@aux{english}{}
\citation{fpp3}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Brent crude oil prices for the period from December 26, 2016 to September 10, 2018 and potential future changes starting from December 12, 2017.\relax }}{3}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:potential_future}{{1}{3}{Brent crude oil prices for the period from December 26, 2016 to September 10, 2018 and potential future changes starting from December 12, 2017.\relax }{figure.caption.2}{}}
\citation{normflow2021}
\citation{rnn2019}
\citation{tsdeeplearning2021}
\citation{MAKRIDAKIS202054}
\citation{m52020}
\citation{m52020}
\citation{introductiondgm2021}
\citation{goodfellow2014}
\citation{choi2018stargan}
\citation{oord2016wavenet}
\citation{koochali2020like}
\citation{lezmi2020improving}
\citation{fpp3}
\citation{lezmi2020improving}
\citation{quantgan2020}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Generated time series. On the left are highly correlated time series, the correlation is 0.98. On the right are highly negatively correlated time series, the correlation is -0.98. \relax }}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig:correlation_ts}{{2}{4}{Generated time series. On the left are highly correlated time series, the correlation is 0.98. On the right are highly negatively correlated time series, the correlation is -0.98. \relax }{figure.caption.3}{}}
\citation{mts2007}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{5}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Probabilistic modeling}{5}{subsection.2.1}\protected@file@percent }
\citation{mts2007}
\citation{mts2007}
\citation{mts2007}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Vector autoregressive model}{6}{subsection.2.2}\protected@file@percent }
\citation{fpp3}
\citation{guerrero93}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Transformations for stationarity}{7}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Deep generative models}{7}{subsection.2.3}\protected@file@percent }
\newlabel{deepgenmodels}{{2.3}{7}{Deep generative models}{subsection.2.3}{}}
\newlabel{eq:gen}{{1}{7}{Deep generative models}{equation.2.1}{}}
\citation{introductiondgm2021}
\citation{tsdeeplearning2021}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Direct parametric approach}{8}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Generative adversarial network}{8}{subsubsection.2.3.2}\protected@file@percent }
\newlabel{gan}{{2.3.2}{8}{Generative adversarial network}{subsubsection.2.3.2}{}}
\citation{goodfellow2014}
\citation{koshiyama2019generative}
\citation{koochali2020like}
\citation{normflow2021}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Normalizing flow}{9}{subsubsection.2.3.3}\protected@file@percent }
\citation{normflow2021}
\citation{normflow2021}
\citation{normflow2021}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Loss functions during training GAN\relax }}{10}{figure.caption.4}\protected@file@percent }
\newlabel{fig:gan}{{3}{10}{Loss functions during training GAN\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Architectures for time series forecasting}{10}{subsection.2.4}\protected@file@percent }
\citation{fpp3}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Neural network autoregression}{11}{subsubsection.2.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces An example of the $\text  {NNAR}(p, P, k)_m$ model. The parameters are $p=3$, $P=2$, $k=2$, $m=12$. The predictable horizon $h=1$.\relax }}{12}{figure.caption.5}\protected@file@percent }
\newlabel{fig:nnar}{{4}{12}{An example of the $\text {NNAR}(p, P, k)_m$ model. The parameters are $p=3$, $P=2$, $k=2$, $m=12$. The predictable horizon $h=1$.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Recurrent neural network}{12}{subsubsection.2.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces An example of the RNN model. The predictable horizon $h=1$.\relax }}{12}{figure.caption.6}\protected@file@percent }
\newlabel{fig:lstm}{{5}{12}{An example of the RNN model. The predictable horizon $h=1$.\relax }{figure.caption.6}{}}
\citation{bahdanau2016neural}
\citation{vaswani2017attention}
\citation{tsdeeplearning2021}
\citation{normflow2021}
\citation{BaiTCN2018}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.3}Attention-based model}{13}{subsubsection.2.4.3}\protected@file@percent }
\citation{koochali2020like}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces An example of the multi-head self-attention model. The predictable horizon $h=1$, the window size is 8.\relax }}{14}{figure.caption.7}\protected@file@percent }
\newlabel{fig:attention}{{6}{14}{An example of the multi-head self-attention model. The predictable horizon $h=1$, the window size is 8.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.4}Temporal convolutional network}{14}{subsubsection.2.4.4}\protected@file@percent }
\citation{normflow2021}
\citation{quantgan2020}
\citation{tsgan}
\citation{cganforts}
\citation{gaussiantcn2020}
\citation{deepar}
\citation{multihorizon}
\citation{interpr}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces An example of dilated causal convolutions. The number of convolutions is 4, dilation factor is $2^i$, kernel size is $2$. In this case, the receptive field will be 16. The predictable horizon $h=1$.\relax }}{15}{figure.caption.8}\protected@file@percent }
\newlabel{fig:tcn}{{7}{15}{An example of dilated causal convolutions. The number of convolutions is 4, dilation factor is $2^i$, kernel size is $2$. In this case, the receptive field will be 16. The predictable horizon $h=1$.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Literature reviews}{15}{section.3}\protected@file@percent }
\citation{BaiTCN2018}
\citation{borovykh2018conditional}
\citation{fpp3}
\citation{gaussiantcn2020}
\citation{Salinas2019HighDimensionalMF}
\citation{normflow2021}
\citation{quantgan2020}
\citation{koochali2020like}
\citation{quantgan2020}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental setup}{16}{section.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The transformation, forecasting, inverse transformation pipeline using the VAR model.\relax }}{16}{figure.caption.9}\protected@file@percent }
\newlabel{fig:transformation_var}{{8}{16}{The transformation, forecasting, inverse transformation pipeline using the VAR model.\relax }{figure.caption.9}{}}
\citation{Koenker2005}
\citation{gaussiantcn2020}
\@writefile{toc}{\contentsline {section}{\numberline {5}Metrics}{17}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Quantile loss}{17}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Autocorrelation loss}{17}{subsection.5.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The results among models, QL \IeC {\textemdash } quantile loss, CL \IeC {\textemdash }\IeC {\nobreakspace  }correlation loss, ACL \IeC {\textemdash }\IeC {\nobreakspace  }autocorrelation loss. The smaller the better. The row "Train-test" shows the discrepancy between the train and test sets by metrics CL and ACL.\relax }}{18}{table.caption.10}\protected@file@percent }
\newlabel{table:1}{{1}{18}{The results among models, QL — quantile loss, CL — correlation loss, ACL — autocorrelation loss. The smaller the better. The row "Train-test" shows the discrepancy between the train and test sets by metrics CL and ACL.\relax }{table.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Correlation loss}{18}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Dataset}{18}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Retail Trade, Australia}{18}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Numerical results}{18}{section.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Example of the forecast via VAR model\relax }}{19}{figure.caption.11}\protected@file@percent }
\newlabel{fig:var}{{9}{19}{Example of the forecast via VAR model\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Example of the forecast via Transformer-RealNVP model\relax }}{19}{figure.caption.12}\protected@file@percent }
\newlabel{fig:transformer_realnvp}{{10}{19}{Example of the forecast via Transformer-RealNVP model\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Example of the forecast via Gaussian TCN model\relax }}{19}{figure.caption.13}\protected@file@percent }
\newlabel{fig:gaussian_tcn}{{11}{19}{Example of the forecast via Gaussian TCN model\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Example of the forecast via TCN cGAN model\relax }}{19}{figure.caption.14}\protected@file@percent }
\newlabel{fig:tcn_cgan}{{12}{19}{Example of the forecast via TCN cGAN model\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{19}{section.8}\protected@file@percent }
\bibdata{main}
\bibcite{bahdanau2016neural}{{1}{2016}{{Bahdanau et~al.}}{{Bahdanau, Cho, and Bengio}}}
\bibcite{BaiTCN2018}{{2}{2018}{{Bai et~al.}}{{Bai, Kolter, and Koltun}}}
\bibcite{borovykh2018conditional}{{3}{2018}{{Borovykh et~al.}}{{Borovykh, Bohte, and Oosterlee}}}
\bibcite{gaussiantcn2020}{{4}{2020}{{Chen et~al.}}{{Chen, Kang, Chen, and Wang}}}
\bibcite{choi2018stargan}{{5}{2018}{{Choi et~al.}}{{Choi, Choi, Kim, Ha, Kim, and Choo}}}
\bibcite{multihorizon}{{6}{2019}{{Fan et~al.}}{{Fan, Zhang, Pan, Li, Zhang, Yuan, Wu, Wang, Pei, and Huang}}}
\bibcite{deepar}{{7}{2017}{{Flunkert et~al.}}{{Flunkert, Salinas, and Gasthaus}}}
\bibcite{goodfellow2014}{{8}{2014}{{Goodfellow et~al.}}{{Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio}}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Example of the forecast via VEC-LSTM model\relax }}{20}{figure.caption.15}\protected@file@percent }
\newlabel{fig:vec_lstm}{{13}{20}{Example of the forecast via VEC-LSTM model\relax }{figure.caption.15}{}}
\bibcite{guerrero93}{{9}{1993}{{Guerrero}}{{}}}
\bibcite{rnn2019}{{10}{2019}{{Hewamalage et~al.}}{{Hewamalage, Bergmeir, and Bandara}}}
\bibcite{fpp3}{{11}{2021}{{Hyndman and Athanasopoulos}}{{}}}
\bibcite{Koenker2005}{{12}{2005}{{Koenker}}{{}}}
\bibcite{koochali2020like}{{13}{2020}{{Koochali et~al.}}{{Koochali, Dengel, and Ahmed}}}
\bibcite{koshiyama2019generative}{{14}{2019}{{Koshiyama et~al.}}{{Koshiyama, Firoozye, and Treleaven}}}
\bibcite{lezmi2020improving}{{15}{2020}{{Lezmi et~al.}}{{Lezmi, Roche, Roncalli, and Xu}}}
\bibcite{tsdeeplearning2021}{{16}{2021}{{Lim and Zohren}}{{}}}
\bibcite{interpr}{{17}{2020}{{Lim et~al.}}{{Lim, Arik, Loeff, and Pfister}}}
\bibcite{mts2007}{{18}{2007}{{Ltkepohl}}{{}}}
\bibcite{MAKRIDAKIS202054}{{19}{2020{}}{{Makridakis et~al.}}{{Makridakis, Spiliotis, and Assimakopoulos}}}
\bibcite{m52020}{{20}{2020{}}{{Makridakis et~al.}}{{Makridakis, Spiliotis, Assimakopoulos, Chen, Gaba, Tsetlin, and Winkler}}}
\bibcite{oord2016wavenet}{{21}{2016}{{Mogren}}{{}}}
\bibcite{normflow2021}{{22}{2021}{{Rasul et~al.}}{{Rasul, Sheikh, Schuster, Bergmann, and Vollgraf}}}
\bibcite{introductiondgm2021}{{23}{2021}{{Ruthotto and Haber}}{{}}}
\bibcite{Salinas2019HighDimensionalMF}{{24}{2019}{{Salinas et~al.}}{{Salinas, Bohlke-Schneider, Callot, Medico, and Gasthaus}}}
\bibcite{cganforts}{{25}{2020}{{Smith and Smith}}{{}}}
\bibcite{vaswani2017attention}{{26}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{quantgan2020}{{27}{2020}{{Wiese et~al.}}{{Wiese, Knobloch, Korn, and Kretschmer}}}
\bibcite{tsgan}{{28}{2019}{{Yoon et~al.}}{{Yoon, Jarrett, and van~der Schaar}}}
